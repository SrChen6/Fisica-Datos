By: Haokang Chen

# Estimadors
## Plug in
Sigui una variable aleatòria $X\sim N(\mu, \sigma^{2}=1)$ i unes vaiid $X_{1},\dots,X_{n}$ m.a.s. de $X$. $$\mu=E(X)=\int_{\mathbb R} xF'(x)dx=\Phi(F)$$ on $F$ és la funció real, la que segueixen les mostres). $F_{n}$ és la distribució empírica, la que utilitzem per aproximar la distribució. $$\hat\mu=E(X_{e})=\Phi(F_{n})$$
- Moment d'ordre $k$: $$m_{k}=\frac 1 n \sum\limits_{i=1}^{n}X_{i}^{k}$$ on $X_{i}$ són les mostres de les quals disposem.

Exemple: $X\sim N(\mu=50, \sigma^{2})$: $$Var(X)=E(X^{2})-E^{2}(X)=\mu_{2}-\mu_{1}^{2}$$ on $\mu_{k}=E(X^{k})$. Per tant: $$\hat\sigma^{2}=\frac 1n \sum\limits_{i=1}^{n}(X_{i}-\overline X_{n})^{2}$$
Sabent que $\mu_{1}=E(X),\sigma^{2}(E[(X_{E(X))^2}])$: $$\mu_{2}=\sigma^{2}+\mu_{1}^{2}$$

## MLE: Estimadors de màxima versemblança
Coneixements previs:
- $Argmax(A)$ retorna la posició del màxim del conjunt A. $$argmax(f(x))=argmax(\log(f(x)))$$
Sigui una mostra aleatòria simple $X_{1},\dots, X_{n}$, de la qual sabem la distribució però no la probabilitat, l'estimador $L(p,X_{1},\dots,X_{n})$ ens permet trobar la $p$.

Per exemple, per $Ber(p)$, $L(p=\frac 1 2, X_{1},\dots,X_{n})=\alpha, L(p=\frac 1 6,X_{1},\dots, X_{n})=\beta$. El màxim entre $\alpha, \beta$ serà el més versemblant amb la mostra.

- Teorema: sigui un estimador $\hat\theta$, amb una funció $\psi(\theta)$ es pot obtenir un estimador $\hat\psi$
Per exemple, $$\hat\mu=\frac 1 n\sum\limits_{i=1}^{n}x_{i}$$ amb una transformació trobem que $$\hat\sigma^{2}=\frac 1 n\sum\limits_{i=1}^{n}(x_{i}-\hat\mu )^{2}$$
# Propietats d'estimadors
