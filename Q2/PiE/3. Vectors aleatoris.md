Una variable aleatòria n-dimensional (AKA vector aleatori) sobre un espai de probabilitats ($\Omega,\mathcal A,\mathbb P$) és $X=(X_{1},\dots,X_{n})$ amb funció $$X:\Omega\to\mathbb R^{n}$$ $$\omega\to(X_{1}(\omega),\dots,X_{n}(\omega))$$ on $X_{i}$ són variables aleatòries

Sigui $X$ una variable aleatòria n-dimensional, la funció de distribució $:F_{X}:\mathbb R^{n}\to\mathbb R$ tal que $F_{X}(x_{1},\dots,x_{n})=\mathbb P(X_{1}\leq x_{1},\dots,X_{n}\leq x_{n})$ 

Observació: Molts cops parlem de variables aleatòries bidimensionals ($n=2$), escriurem $(X,Y)$ 

Observació: Sigui $F_{X}$ una funció d'acumulació, $$\mathbb P(X,Y)\in((a,b]\times(c,d])=F(b,d)-F(b,c)-F(a,d)+F(a,c))$$ que és fonamentalment el principi d'inclusió-exclusió

# Vectors aleatoris discrets
Un vector aleatori discret és un vector aleatori $X=(X_{1},\dots,X_{n})$ on totes les $X_{i}$ són discretes

La funció de probabilitat conjunta és $$\mathbb P(X_{1}=x_{1},\dots,X_{n}=x_{n})$$ per $\vec x\in\mathbb R^{n}$

La i-èssima marginal de $X$ és la variable aleatòria $X_{i}$ tal que $$P(X_{i}=x_{i})=\sum\limits_{x_{1},\dots,\hat x_{i},\dots,x_{n}}\mathbb P(X_{1}=x_{1},\dots,\hat{X_{i}=x_{i}},\dots,X_{n}=x_{n})$$ Per tant, es fixa un valor de X i es sumen tots els altres components (al $X_{i}=x_{i}$ el barret hauria d'incloure tota l'expressió, no se com s'arregla)

## Independència
Siguin $X,Y$ dues variables aleatòries amb distribució conjunta $(X,Y)$, $X$ i $Y$ són independents si $$\mathbb P(X=x,Y=y)=\mathbb P(X=x)·\mathbb (Y=y)\quad \forall x,y$$ Generalitzant, $X_{1},\dots,X_{n}$ són independents si $$\mathbb P(\bigcap_{i\in S}\{X_{i}=x_{i}\})=\prod_{i\in S}\mathbb P(X_{i}=x_{i})$$
## Model multinomial
Es realitzen $n$ experiments i tenim $k$ possibles resultats per cada experiment

$X=(X_{1},\dots,X_{k})$ és multinomial amb paràmetres $n\in\mathbb N$, $p_{1},\dots,p_{x}\in[0,1]$ tal que $p_{1}+p_{k}=1$ si $$\mathbb P(X=(n_{1},\dots,n_{k}))=\binom n {n_{1},\dots,n_{k}}p_{1}^{n_{1}}\dots p_{k}^{n_{k}}$$ on $\binom n {n_{1},\dots,n_{k}}=\frac{n!}{n_{1}!\dots n_{k}!}$ és el coeficient multinomial
S'escriu $X\sim Multi(n;p_{1},\dots,p_{k})$ quan és així
- Si $k=2, Multi(n;p_{1},p_{2})=Bin(n,p_{1})$
- Si $k=4,Multi(n;p_{1},p_{2},p_{3})=Tri(n;p_{1},p_{2},p_{3})$ 

# Vectors aleatoris continus
Principalment en el cas $n=2$ 

$(X,Y)$ és una variable aleatòria contínua si $\exists f_{X,Y}:\mathbb R^{2}\to\mathbb R$ (densitat) tal que $$F_{X,Y}(x,y)=\int_{-\infty}^{x}\int_{-\infty}^{y}f_{X,Y}(st)dsdt$$ on $\mathbb P((X,Y)\in S)=\int_{S}f_{X,Y}dS$ 
Les funcions de densitat marginal són $$f_{X}(x)=\int_{-\infty}^{\infty}f_{X,Y}(x,y)dy,\quad f_{Y}(y)=\int_{-\infty}^{\infty}f_{X,Y}(x,y)dx$$
## Independència
Siguin $X,Y$ dues variables aleatòries contínues, són independents si $$f_{X,Y}(x,y)=f_{X}(x)·f_{Y}(y)\quad\forall x,y$$

## Model uniforme
Sigui $S\subseteq\mathbb R^{n}, X$ és uniforme sobre $S$ si $$f_{X}(x)=\begin{cases}
\frac 1{vol(S)}\mbox{ si }x\in S \\
0\mbox{ si }x\notin S
\end{cases}$$
S'escriu $X\sim U(S)$ si és així

# Covarància i correlacions
El moment d'ordre $(r,s)$ d'un vector aleatori en $\mathbb R^{2}$ és $$E(x^{r}y^{s})=\begin{cases}
\sum\limits_{x}\sum\limits_{y}x^{r}y^{s}\mathbb P(X=x,Y=y)\mbox{ si és dicret} \\
\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}x^{r}y^{s}f_{X,Y}(x,y)dxdy
\end{cases}$$

I el moment centrat d'ordre $(r,s)$ és $$E((X-E(X))^{r}(Y-E(Y))^{s})$$ on:
- Si $s=0,E(x^{r})$ és el moment d'ordre $r$ de la marginal $X$
- Si $r=0E(y^{s})$ és el moment d'ordre $r$ de la marginal $Y$

## Covariància
El moment $(1,1)$ centrat: $$E((X-E(X))(Y-E(Y)))\equiv Cov(X,Y)$$
Observació: $(2,0)=Var(X),(1,1)=Cov(X,Y),(0,2)=Var(Y)$

Propietats: Siguin $X,Y,Z$ variables aleatòries
- $Cov(X,Y)=E(X,Y)-E(X)E(Y)$
- $Cov(X,Y)=Cov(Y,X)$
- $Cov(X+Y,Z)=Cov(X,Z)+Cov(Y,Z)$
- $Cov(\lambda X, Y)=\lambda Cov(X,Y)$
- $Cov(X,X)=Var(X)$
- $Var(X+Y)=Var(X)+Var(Y)+2Cov(X,Y)$

## Coeficient de correlació $\rho_{XY}$ 
AKA $\rho$ de Pearson: $$\rho_{XY}=\frac{Cov(X,Y)}{\sqrt{Var(X)·Var(Y)}}$$
Proposició: $|\rho_{XY}|\leq 1,|\rho_{XY}|=1\implies Y=aX+b$ 

![[SmartSelect_20240315_133148_Samsung Notes.jpg]]
Per $\rho_{XY}=0$ són variables incorrelades 
Per $\rho_{XY}\leq 0$ hi ha una correlació negativa
Per $\rho_{XY}\geq0$ hi ha una correlació positiva
Per tant, $\rho_{XY}$ mesura la dependència lineal de les variables aleatòries

Observació: $X,Y$ independents$\implies$ $X,Y$ incorrelades


# Distribucions i esperança condicionada
## Cas discret
Siguin $X,Y$ variables aleatòries discretes
$$\mathbb P(X=x|Y=y)=\frac{\mathbb P(X=x,Y=y)}{\mathbb P(Y=y)}$$ on $\mathbb P(Y=y)\neq 0$ 
Per cada $y$ tenim una variable aleatòria $X|Y=y$ a la que podem calcular $E$: $$E(X|Y=y)=\sum\limits_{x}x\mathbb P(X=x|Y=y)$$ per tant, l'esperança condicionada $E(X|Y)$ és una variable aleatòria $g(Y)$ que proporciona valors diferents de $E$ segons el valor de $Y$ 

- Llei de l'esperança iterada: $$E(E(X|Y))=E(X)$$

## Cas continu
Siguin $X,Y$ variables aleatòries contínues amb $f_{Y}(y)>0$, la densitat de $x$ condicionada a la $Y=y$ és $$f_{X|Y}(x|y)=\frac{f_{X,Y}(x,y)}{f_{Y}(y)}$$ i analògicament: $$F_{X|Y}(x|y)=\int_{-\infty}^{x}f_{X|Y}(t|y)dt$$
Per cada $y$ tenim una variable aleatòria $X|Y=y$ a la que podem calcular $E$: $$E(X|Y=y)=\int_{\mathbb R}xf_{X|Y}(x|y)dx$$ per tant l'esperança condicionada $E(X|Y)$ és una variable aleatòria $g(Y)$ que proporciona valors diferents de $E$ segons el valor de $Y$ 

- Llei de l'esperança iterada: $$E(E(X|Y))=E(X)$$
# Model normal multivariant
## Matriu de covariàncies
Matriu $K_{X}$ on $$(K_{X})_{ij}=Cov(X_{i},X_{j})$$ $$K_{X}=E((X-E(X))^{T}(X-E(X)))$$
Si $K_{X}$ és diagonal i semi-definida positiva ($\forall \lambda\ VAP,\lambda\geq 0$), existeix una matriu $M$ tal que $$K_{X}=MM^{T}$$ sempre i quan no sigui singular ($det(K_{X}\neq 0$)

## Normal multivariant
$X=(X_{1},\dots,X_{n})$ és una normal multivariant de dimensió $n$ amb paràmetres $\overline\mu\in\mathbb R^{n}$ i $\Sigma$ matriu semidefinida positiva si $$f_{X}(x)=\frac{1}{\sqrt{(2\pi)^{n}det(\Sigma)}}\exp(-\frac{(x-\overline\mu)^{T}\Sigma^{-1}(x-\mu)}{2})\quad\forall x\in\mathbb R^{n}$$
Si és així, s'escriu $X\sim N(\overline\mu,\Sigma)$ 

## Propietats
Sigui $X\sim N(\overline\mu,\Sigma)$:
1. $E(X)=\overline\mu,K_{X}=\Sigma$
2. $X=MZ+\mu,\quad Z\sim N(\overline 0, Id)$ 
3. $X_{i}\sim N(\mu_{i},\Sigma_{ii})\quad\forall i\in[n]$ 
4. $\Sigma_{ij}=0\implies X_{i},X_{j}$ independents
5. $(X_{i}|X_{j}=x_{j})\sim N(\mu_{i}+\rho_{ij}\frac{\sigma_{i}}{\sigma_{j}}(x_{j}-\mu_{j}),(1-\rho_{ij})^{2}\sigma_{i}^{2}),\quad\sigma_{i}=+\sqrt{\Sigma_{ij}},\rho_{ij}=\frac{\Sigma_{ij}}{\sigma_{i}\sigma_{j}}$ 

## Composició de vectors aleatoris
Sigui $X=g(Z)=MZ+\mu,g:\mathbb R^{n}\to\mathbb R^{n}$ $$f_X(x)=|\mathcal J(g^{-1}(x))|f_{Z}(g^{-1}(x))$$ on $Z=g^{-1}(X)=M^{-1}(X-\mu)$ i $$\large\mathcal J(g^{-1}(X))=det
\begin{pmatrix}
\frac{\partial g_{1}}{\partial X_{1}}&\dots&\frac{\partial g_{1}}{\partial X_{n}} \\ 
\vdots&\ddots&\vdots \\ 
\frac{\partial g_{n}}{\partial X_{n}}&\dots&\frac{\partial g_{n}}{\partial X_{n}}
\end{pmatrix}$$ 