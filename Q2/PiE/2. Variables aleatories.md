[[PIE_DSE_Tema2.pdf]] 

Una variable aleatòria (VA) sobre un espai de probabilitat $(\Omega,\mathcal A,\mathbb P)$ és una funció: $$X:\Omega\to\mathbb R|X^{-1}((-\infty,x])=\{\omega\in\Omega|X(\omega)\leq x\}$$ on $\omega$ és element de $\mathcal A$ (mesurable)

# VA indicadors
Sigui $A\subseteq\Omega$ , $\mathcal A=\{\emptyset,A,\overline A,\Omega\}$, $\sigma$ = àlgebra de Bernoulli:
$$\mathbb P(A)=p\in[0,1]$$
- Sigui $X=\mathbb 1_{A}$:
	$$X(\omega)=\begin{cases}1 \mbox{ for } \omega\in A\\0\mbox{ for }\omega\notin A\end{cases}$$ $X$ és una VA perquè
$$X^{-1}((-\infty,x])=\begin{cases}
\emptyset \mbox{ for }x<0 \\
\overline A\mbox{ for }0\leq x<1 \\
\Omega \mbox{ for }x\geq 1
		\end{cases}$$
i $\{\emptyset,\overline A,\Omega\}\in\mathcal A$ 

- Cal remarcar que $X,Y,Z$ son VA i $x,y,z$ són valors reals 
	$Im(X)\equiv X(\Omega)$

Simplificant la notació: 
	$S\subseteq\mathbb R,X^{-1}(S)$ són els elements $\omega\in \Omega$ tal que $X(a)\in S$ $$\mathbb P(X^{-1}(S))\to\mathbb P(X\in S)$$

## Funció de distribució
Donada $X$ VA en un espai de probabilitat $(\Omega, \mathcal A,\mathbb P)$, la seva funció de distribució (AKA funció d'acumulació) és $$F_{X}:\mathbb R\to \mathbb R|F_{X}(x)=\mathbb R(X\leq x)$$ Per exemple: $X=\mathbb 1_{A},\mathbb P(A)=p$, 
$$F_{X}(x)=\begin{cases}
0 \mbox{ for } x<0 \\
1-p \mbox{ for } 0\leq x<1 \\
1 \mbox{ for } x\geq 1
\end{cases}$$

Es compleix:
1. $\lim\limits_{x\to\infty}F_{X}(x)=1,\lim\limits_{x\to-\infty}=0$ 
2. $F_{X}(x)$ és no decreixent: $$\forall x\leq y,F_{X}(x)\leq F_{X}(y)$$
3. $F_{X}(x)$ és sempre contínua per la dreta: $$\forall x\in\mathbb R,\lim_{y\to x^{+}}F(y)=F(x)$$
4. La probabilitat d'un interval és: $$\forall a,b\in\mathbb R,a\leq b,\mathbb P(a<x\leq b)=F_{X}(b)-F_{X}(a)$$
5. La probabilitat d'un punt amb discontinuïtat de salt és: $$\forall a\in\mathbb R,\mathbb P(X=a)=F_{X}(a)-\lim_{x\to a^{-}}F_{X}(x)$$
# Variables aleatòries discretes
$X$ és VA discreta si $X(\Omega)$ és comptable/numerable

La funció de probabilitat és $$\mathbb P_{X}(x)=\mathbb P(X=x)$$
Propietats:
1. $P_{X}(x)\geq 0$
2. $\sum\limits_{x\in X(\Omega)}P_{X}(x)=1$ 

Observació: Qualsevol VA discreta es pot escriure com a suma de VA indicadores: $$X=\sum\limits_{x\in X(\omega)}x\mathbb 1_{X=x}$$
Proposició: Sigui $X$ una VA discreta i $g:\mathbb R\to\mathbb R$ que satisfà que $$\forall x\in\mathbb R\quad g^{-1}(-\infty,x)\in\mathcal B\text{ (borelià) }\implies g(x)\text{ és VA discreta }$$ 
# Models de probabilitat discreta

## Model de Bernoulli
$X$ és de Bernoulli amb paràmetre $p\in[0,1]$ si $$\mathbb P(X=0)=1-p,\mathbb R(X=1)=p$$
Denotat com $X\sim Be(p)$ 

## Model Binomial
Fem $n$ experiments independents i cadascun amb probabilitat d'èxit $p$ i comptem quants tenen èxit, $X$ és binomial de paràmetres $n\in\mathbb N$ i $p\in(0,1]$ si $$\mathbb P(X=k)=\binom n k p^{k}(1-p)^{n-k}$$ per $k\in\{1,\ldots,n\}$ 
Denotat com $X\sim Bin(n,p)$

Taula de Galton: aproximació física d'una distribució binomial.

Proposició: si $X\sim Bin(n,p)$: $$\mathbb P(X=k)\leq\mathbb P(X=k+1)\mbox{ if }k\leq(n+1)p-1\sim np$$ $$\mathbb P(X=k)\geq\mathbb P(X=k+1)\mbox{ if }k\geq(n+1)p-1\sim np$$ Per tant la distribució binomial és una funció unimodal (només creix/decreix)

## Model de Poisson
El nombre d'èxits quan hi ha moltíssims experiments però la probabilitat és molt petita

$X$ és de Poisson amb paràmetre $\lambda>0$ (intensitat) si $$\mathbb P(X=k)=e^{-\lambda}\frac{\lambda^{k}}{k!},\quad k\geq 0,\quad X\sim Po(p)$$ Per tant, el model de Poisson és un model binomial on $n\to\infty$ i $p\to 0$ 

## Model Geomètric
El nombre de vegades que s'ha de repetir un experiment fins que tenim èxit.

$X$ és geomètrica de paràmetre $p\in(0,1]$ si $$\mathbb P(X=k)=p(1-p)^{k-1},\quad k\in\mathbb N,\quad X\sim Geom(p)$$
Observacions:
- $\mathbb P(X=k)=\mathbb P(k\text{ fracassos})=(1-p)^{k}$
- A vegades direm geomètrica a la variable que compta el nombre de fracassos ($X\sim Geom_{o}(p)$)
- Memoryless: $\mathbb P(X>r+s|X>s)=\mathbb P(X>r)$ 
	Si una variable aleatòria discreta no té memòria ha de ser una geomètrica

## Model Binomial Negatiu

Nombre d'experiments amb probabilitat d'èxit $p$ que hem de fer abans de tenir $r$ èxits.

$X$ és binomial negativa de paràmetres $r\in\mathbb N$ i $p\in(0,1]$ si $$\mathbb P(X=k)\binom{k-1}{r-1}p^{r}(1-p)^{k-r},\quad k\geq r,\quad X\sim NegBin(r,p)$$
Observació: Podem entendre-la com la suma de $r$ variables geomètriques independents

## Model hipergeomètric
Amb $n$ individus dels quals $m$ són bons, el nombre de bons si en seleccionem $r$ dels $n$ sense repetició, similar a una $Bin(n,p)$ sense repetició

$X$ és hipergeomètric amb paràmetres $n,m,r\in\mathbb N$ si $$\mathbb P(X=k)=\frac{\binom m k\binom{n-m}{r-k}}{\binom n r}$$
Observació: $$\max\{0,m+r-n\}\leq k\leq\min\{m,r\}$$

## Model uniforme discret
Distribució que obtenim al tirar un nombre aleatori on tots tenen la mateixa probabilitat de sortir

$X$ es uniforme de paràmetre $n\in\mathbb N$ si $$\mathbb P(X=k)=\frac 1 n,\quad\forall k\in[n],\quad X\sim U(n)\equiv Unif(n)$$
Observació: no es pot obtenir una variable aleatòria uniforma en $\mathbb N$ 

# Esperança i moments
Mitjana: $\overline x=\frac1 n\sum\limits_{i=1}^{n}x_{i}$, frequencialment: $\overline x=\sum\limits_{k}k\frac{n_{k}} n$ on
Esperança: $\mu_{x}=\mathbb E(X)=\sum\limits_{k\in X(\omega)}k·\mathbb P(X=k)$ 
	Similar al centre de masses: és el millor valor constant per aproximar la variable aleatòria
	Observació: pot ser que $\sum\limits\mathbb E(X)$ no fos absolutament convergent. En aquest cas $\mathbb E$ no està definida


| $X$                | $\mathbb E(X)$ |
| ------------------ | -------------- |
| $Be(p)$            | $p$            |
| $Bin(n,p)$         | $np$           |
| $Po(\lambda)$      | $\lambda$      |
| $Geom(p)$          | $\frac 1 p$    |
| $NegBin(r,p)$      | $\frac r p$    |
| $HyperGeom(n,m,r)$ | $\frac{mr} p$  |
| $U(n)$             | $\frac{n+1} 2$ |
## Suma de variables aleatòries
$Z=X+Y$ té funció de probabilitat $$\mathbb P(Z=k)=\sum\limits_{i}\mathbb P(X=i,Y=k-i )$$
- Linealitat de l'esperança: $X,Y,\quad E(X),E(Y)<\infty$ 
	- $E(X+Y)=E(X)+E(Y)$
	- $\lambda\in\mathbb R,E(\lambda X)=\lambda E(X)$ 

- Desigualtat de Markov: sigui $X$ una variable aleatòria no negativa $$\forall a>0,\quad \mathbb P(x\geq a)\leq\frac{E(X)}{a}$$

Proposició: sigui $X$ una variable aleatòria discreta, $g:\mathbb R\to\mathbb R$ mesurable $$E(g(X))=\sum\limits_{k\in X(\Omega)}g(k)\mathbb P(X=k)$$
# Moment d'ordre
Sigui $X$ una variable aleatòria discreta, el moment d'ordre $m$ és $$E(X^{m})=\sum\limits_{k\in X(\Omega)}k^{m}\mathbb P(X=k)$$
- Moment centrat: $$E((X-E(X))^{m})=\sum\limits_{k\in X(\Omega)}(k-E(X))^{m}\mathbb P(X=k)$$
Per $m=1$ és l'esperança
Per $m=2$ és la **variància** $$\sigma_{X}^{2}=Var(X)=E((X-E(X))^{2})=\sum\limits_{k\in X(\Omega)}(k-E(X))^{2}\mathbb P(X=k)$$ d'on podem definir la desviació típica $$\sigma_{X}=+\sqrt{Var(X)}$$
- Càlcul de la variancia: $$Var(X)=E(X^{2})-E(X)^{2}$$

| Distribució            | Variancia                       |
| ---------------------- | ------------------------------- |
| $X\sim Be(p)$          | $pq$                            |
| $X\sim Bin(n,p)$       | $npq$                           |
| $X\sim Po(\lambda)$    | $\lambda$                       |
| $X\sim Geom(p)$        | $\frac{q}{p^2}$                 |
| $X\sim NegBin(r,p)$    | $\frac{rq}{p^2}$                |
| $X\sim HypGeom(n,m,r)$ | $\frac{rm(n-m)(n-r)}{n^2(n-1)}$ |
| $X\sim U(n)$           | $\frac{(n^2 -1)} {12}$          |

Propietats de la variancia: 
- $$Var(\lambda X)=\lambda^{2}Var(X)$$
- $$Var(X+Y)\neq Var(X)+Var(Y)$$

### Desigualtat de Chevyshev
Sigui $X$ una variable aleatòria, $a>0$ $$\mathbb P(|X-E(X)|\geq a)\leq\frac{Var(X)}{a^{2}}$$
- Chevyshev vs Markov

| Chevyshev                             | Markov                           |
| ------------------------------------- | -------------------------------- |
| Cua superior i inferior               | Cua superior o inferior          |
| $\frac 1 {a^2}\implies$ cau més ràpid | $\frac 1 a\implies$ cau més lent |
| Costa més calcular-lo                 | Costa menys calcular-lo          |
## Funció generadora de moments
$$M_{X}(t)=\mathbb E(e^{kt})=\sum\limits_{k}e^{kt}\mathbb P(X=k)$$
Observació: Si $X, Y$ compleixen $M_{X}(t)=M_{Y}(t)$ per $t$ en una bola oberta al voltant de $t=0$, llavors $X,Y$ tenen la mateixa distribució

# Variables aleatòries contínues
Una variable aleatòria és contínua si $\exists f_{X}:\mathbb R\to\mathbb R$ (densitat) tal que $$F_{X}(x)=\mathbb P(X\leq x)=\int_{-\infty}^{x}f_{X}(t)dt$$
Observació: Si $X$ és contínua, com $F_{X}$ té antiprimitiva, aleshores $F_{X}$ és contínua ($f_{X}$ pot ser discontínua)

Observació: $$\mathbb P(X\leq x)=\mathbb P(X<x)$$
Observació: pel Teorema Fonamental del Calcul, $a\in \mathbb R,$ $h$ petita: $$f(a)=\lim_{h\to 0}\frac{\mathbb P(a<X<a+h)}h$$
Observació: $$\mathbb P(a<X<b)=F_{X}(b)-F_{X}(a)=\int_{a}^{b}f_{X}(t)dt$$
Propietats: $f_{X}$ compleix
1. No és negativa: $f_{X}(x)\geq 0\quad \forall x$
2. $\int_{-\infty}^{\infty}f_X(t)dt=1$ 

# Models de probabilitat contínues
## Model uniforme
$X$ és Uniforme amb paràmetres $a,b\in\mathbb R$ amb $a\leq b$ si $$f_{X}(x)=\frac 1 {b-a}\mathbb 1_{[a,b]}(x)=\begin{cases}
\frac{1}{b-a}\mbox{ si }x\in[a,b] \\
0\mbox { si }x\notin[a,b]
\end{cases}$$
La seva funció de probabilitat acumulada és $$F_{X}(x)=\begin{cases}
0\mbox{ si }x<a \\
\frac{x-a}{b-a}\mbox{ si }a\leq x\leq b \\
1\mbox{ si }x>b
\end{cases}$$
S'escriu $X\sim U([a,b])$ si és així

Observació: Una distribució uniforme contínua és el límit a infinit de uniformes discretes

## Model exponencial
$X$ és exponencial de paràmetre $\lambda$ si $$f_{X}(x)=\lambda e^{-\lambda x}\quad \forall x\geq 0$$
La seva funció de probabilitat acumulada és $$F_{X}(x)=1-e^{-\lambda x}$$
Observació: una distribució exponencial és el límit  de geomètriques quan els intervals de temps són $dt$ i $p\to 0$ 

Proposició: $X\sim Exp(\lambda)$ $$\mathbb P(X>s+t|X>s)=\mathbb P(X>t)\quad \forall s,t$$
Observació: $Exp(\lambda)$ i $Po(\lambda)$ estan relacionades

Observació: és memoryless

S'escriu $X\sim Exp(\lambda)$ si és així

## Model normal
$X$ és una distribució normal amb paràmetres $\mu\in\mathbb R$ i $\sigma>0$ si $$f_{X}(x)=\frac 1 {\sqrt{2\pi}\sigma}\exp(-\frac{(x-\mu)^{2}}{2\sigma^{2}})\quad\forall x\in\mathbb R$$ on $\mu$ canvia el lloc on es centra $f_{X}(x)$ i $\sigma$ canvia la concentració del pic de $f_{X}(x)$

S'escriu $X\sim N(\mu,\sigma)$ o $X\sim N(\mu,\sigma^{2})$ quan és així

Cas especial: Normal estàndard ($Z\sim N(0,1)$), $\mu=0,\sigma=1$, que ens dona: $$f_{Z}(z)=\frac1{\sqrt{2\pi}}\exp(-\frac{z^{2}}2)$$
La seva funció de probabilitat acumulada no es por representar amb funcions elementals, per tant la definim com: $$\Phi(z)=\int_{-\infty}^{z}\frac 1{\sqrt{2\pi}}\exp({-\frac{t^{2}}2})dt$$
Observació: les cues de la normal tendeixen a 0 molt ràpid (més ràpid que l'exponencial negativa)

Proposició: podem estandaritzar les normes: $$X\sim N(\mu,\sigma)\to Y=\frac{X-\mu}{\sigma}\sim N(0,1)$$
- Teorema de de Moivre-Laplace
Si $X\sim Bin(n,p)$ i $Y\sim N(\mu=np,\sigma^{2}=np(1-p))$ aleshores per $k$ comparable amb $np$ $$\mathbb P(X=k)=\binom n k p^{k}(1-p)^{n-k}\sim\frac 1 {\sqrt{2\pi}}\exp(-\frac{(x-\mu)^{2}}{2\sigma^{2}})=\mathbb P(Y=k)$$

Propietat: $X\sim N(\mu,\sigma^{2}), Y\sim N(m,s^{2})$, llavors $$X+Y\sim N(\mu + m,\sigma ^{2}+s^{2})$$ 

## Model Gamma 
$X$ és de Gamma amb paràmetre $\alpha$ (shape) i $\lambda$ (rate) si $$f_{X}(x)=\frac{\lambda^{\alpha}}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x}\quad\forall x>0$$ on $\Gamma(\alpha)=\int_{0}^{\infty}t^{\alpha-1}e^{-t}dt$ és la funció Gamma d'Euler

Modificar $\alpha$ canvia la funció per valors petits $x$ (a més $\alpha$ menys pic)
Modificar $\lambda$ canvia la funció per valors grans de $x$ (a més $\lambda$ més ràpid decreix)

S'escriu $X\sim Gamma(\alpha, \lambda)$ si és així

Observació: A vegades es fa servir $\theta=\frac 1 \lambda$ (scale) en comptes de $\lambda$ 

Observació: Per $\alpha = 1$ tenim $Gamma(1,\lambda)=Exp(\lambda)$ 
	Per $\alpha\in\mathbb N$ tenim $Gamma(\alpha,\lambda)$ una suma de $n$ exponencials independents

Observació: $Gamma(\alpha,\lambda)$ es pot entendre com un límit de Binomials Negatives

## Model Beta
$X$ és una distribució Beta de paràmetres $\alpha,\beta>0$ si $$f_{X}(x)=\frac1{B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1}\quad x\in[1,0]$$ on $B(\alpha,\beta)$ és la funció beta (normalitzada)

Observació: El rànquing de uniformes és $$X_{1},X_{2}\sim U([0,1])\implies\begin{cases}
X_{min}=\min{X_{1},X_{2}} \\
X_{max}=\max{X_{1},X_{2}}
\end{cases}$$
# Esperança i moments 
Sigui $X$ una variable aleatòria contínua, l'esperança $E(X)$ es defineix com $$E(X)=\int_{-\infty}^{\infty}x·f_{X}(x)dx$$ El moment d'ordre $k$: $$E(X^{m})=\int_{-\infty}^{\infty}x^{m}f_{X}(x)dx$$ i la variancia $$Var(X)=\int_{-\infty}^{\infty}(X-E(X))^{2}f_{X}(x)dx$$ i finalment, la funció generadora de moments: $$M_{X}(t)=E(e^{tX})=\int_{-\infty}^{\infty}e^{tx}f_{X}(x)$$
Totes les propietats de $E(X),Var(X)$ provades en el cas discret també es compleixen: 
- $E(X)$ és lineal
- $Var(X)=E(X^{2})-E(X)^{2}$


| Distrubució               | $E(X)$            | $Var(X)$              |
| ------------------------- | ----------------- | --------------------- |
| $X\sim U([a,b])$          | $\frac{a+b}2$     | $\frac{(b-a)^2}{12}$  |
| $X\sim Exp(\lambda)$      | $\frac 1 \lambda$ | $\frac 1 {\lambda^2}$ |
| $X\sim N(\mu,\sigma^{2})$ | $\mu$             | $\sigma^2$            |
## Funcions de variables aleatòries contínues
Sigui $X$ una variable aleatòria contínua, $g$ diferenciable i estrictament monòtona, $Y=g(X)$ $$f_{Y}(y)=\frac 1 {|g'(x)|}f_{X}(x)$$ on $X=g^{-1}(y)$

Teorema: sigui $X$ una variable aleatòria, $g(X)$ contínua i $g(X)=y$ $$E(g(X))=\int_{-\infty}^{\infty}y·f_{Y}(y)dy=\int_{-\infty}^{\infty}g(x)f_{X}(x)dx$$ 