# En 1 variable
$f:A\in\mathbb R\to\mathbb R,a\in A'$, la derivada se define como: $$f'(a)\equiv\frac{df}{dx}(a):=\lim_{h\to 0}\frac{f(a+h)-f(a)}{h}$$
- Aproximación lineal: $$f(a+h)=f(a)+f'(a)·h+R_{1}(h)=f(a)+f'(a)(x-a)+R_{1}(h)$$ donde $\lim\limits_{h\to0}\frac{R_{1}(h)}{h}=0$ por lo que podemos obtener una recta tangente $$f(a+h)=y=f(a)+f'(a)(x-a)$$
# Diferencial de una función
$f:A\subseteq\mathbb R^{n}\to\mathbb R^{m},a\in A,a\in A'$, $f$ es diferenciable en $a$ si existe una aplicación lineal (aka diferencial de $f$ en $a$:  $$Df(a):\mathbb R^{n}\to\mathbb R^{m},(h_{1},\ldots,h_{n})\equiv h\to Df(a)(h)$$ donde $Df(a)$ es una matrix $n\times m$ 

Debe cumplirse:
1. $f(a+h)=f(a)+Df(a)h+R_{1}(h)$
2. $\lim\limits_{h\to 0}\frac{R_{1}(h)}{||h||}=0$ 
	 Observación: $$0=\lim\limits_{h\to 0}\frac{R_{1}(h)}{||h||}=\lim\limits_{h\to 0}\frac{f(a+h)-f(a)-Df(a)h}{||h||}=0$$

## Casos particulares
$f:A\subseteq \mathbb R^{n}\to\mathbb R$ $$Df(a):\mathbb R^{n}\to\mathbb R$$$$ h\equiv(h_{1},\ldots h_{n})\to\begin{pmatrix}\alpha_{1}&\dots&\alpha_n\end{pmatrix}\begin{pmatrix}h_{1}\\\vdots\\h_{n}\end{pmatrix}=\alpha_{1}h_{1}+\dots+\alpha_{n}h_{n}$$
1. $f(a+h)=f(a)+\alpha_{1}h_{1}+\dots+\alpha_{n}h_{n}+R_{1}(h)\implies x_{n+1}=f(a_{1},\dots,a_{n})+\alpha(x_{1}-a_{1})+\dots+\alpha_{n}(x_{n}-a_n)+R_{1}(h)$ que és un hiperplà en l'espai $\mathbb R^{n}+1$ tangent a $f$ 
2. $\lim\limits_{h\to 0}\frac{R_{1}(h)}{\sqrt(h_{1}^{2}+\dots+h_{n}^{2})}=0$ 

$f:a\subseteq\mathbb R^{n}\to\mathbb R^{m}$ $$Df(a):\mathbb R^{n}\to\mathbb R^{m}$$ $$ h\equiv(h_{1},\ldots h_{n})\to\begin{pmatrix}\alpha_{1}&\dots&\alpha_{1}^{n}\\\vdots&\ddots&\vdots\\\alpha_{m}&\dots&\alpha_{m}^{n}\end{pmatrix}\begin{pmatrix}h_{1}\\\vdots\\h_{n}\end{pmatrix}$$
1. $f(a+h)=f(a)+A_{m\times n}\begin{pmatrix}h_{1}\\\vdots\\h_{n}\end{pmatrix} +R_{1}(h)$ 
2. $\lim\limits_{h\to 0}\frac{R_{1}(h)}{||h||}=0$

Proposición: $f:A\subseteq\mathbb R^{n}\to \mathbb R^{m},a\in A,a\in A'$
- $f$ es diferenciable en $a\iff$ $f_{i}$ es diferenciable en $a\quad\forall i=1,\dots,m$ 

# Derivadas direccionales y parciales
$f:A\subseteq\mathbb R^{n}\to\mathbb R,a\in A,a\in A'$ 

Para encontrar la derivada de $f$ debemos encontrar $A_{m\times n}$. Para ello, debemos empezar recordando que $$0=\lim\limits_{h\to 0}\frac{f(a+h)-f(a)-Df(a)h}{||h||}$$ y rectas que pasan por $h=0;h=tv\equiv(h_{1},\dots,h_{n})=t(v_{1},\dots,v_{n})$.

Operando encontramos que $$\alpha_{i}=\lim\limits_{t\to 0}\frac{f(a_{1},\dots,a_{i}+h,\dots, a_{n})-f(a_{1},\dots,a_{n})}{t}$$

Para $f:A\subseteq\mathbb R^{n}\to\mathbb R^{m},a\in A,a\in A'$ tenemos que $f\equiv(f_{1},\dots,f_{j},\dots,f_{n})$ así que se repite el proceso anterior para todas las $j$ $$\alpha_{j}^{i}=\lim\limits_{t\to 0}\frac{f_{j}(a_{1},\dots,a_{i}+h,\dots, a_{n})-f_{j}(a_{1},\dots,a_{n})}{t}$$ 
- Derivada direccional de $f$ en $a$ según el vector $v$ (si existe): $$D_{v}f(a):=\lim_{t\to 0}\frac{f(a+tv)-f(a)}{t}$$
- Derivada parcial $i$-ésima: derivada direccional de $f$ en $a$ según el vector $e_i$ de la base canónica: $$\frac{\partial f}{\partial x_{i}}(a):==\lim\limits_{t\to 0}\frac{f_{j}(a_{1},\dots,a_{i}+h,\dots, a_{n})-f_{j}(a_{1},\dots,a_{n})}{t}$$

Teorema: Si $f$ es diferenciable en $a$, entonces
1. $Df(a)$ es única
2. Las componentes de la matriz asociada a esa aplicación lineal en la base canónica son las derivadas parciales de $f$ en $a$: $$\large\mathcal Jf(a)=\begin{pmatrix}\frac{\partial f_{1}}{\partial x_{1}}(a)&\dots&\frac{\partial f_{1}}{\partial x_{m}}(a) \\ \vdots&\ddots&\vdots \\ \frac{\partial f_{n}}{\partial x_{1}}(a)&\dots&\frac{\partial f_{n}}{\partial x_{m}}(a)\end{pmatrix}$$ también llamada la matriz Jacobiana de $f$ en $a$ 

Definición: el Hiperplano tangente al graf en $(a,f(a))$ se describe como: $$X_{n+1}=f(a)+Df(a)(x-a)=f(a)+\mathcal Jf(a)(x-a)=f(a)+\sum\limits_{i=1}^{n}\frac{\partial f}{\partial x_{i}}(a)(x_{i}-a_{i})$$ con lo que podemos definir la aproximación lineal: $$f(a+h)\approx f(a)+Df(a)(h)=f(a)+\sum\limits_{i=1}^{n}\frac{\partial f}{\partial x_{i}}(a)(x_{i}-a_{i})$$

Observación: $\exists$ derivadas direccionales no implica que $f$ sea diferenciable en $a$ 

# Vector gradiente
$f:A\subseteq\mathbb R^{n}\to\mathbb R,a\in A,a\in A',f$ diferenciable en $a$, el vector gradiente de $f$ en $a$ es el vector $graf(a)\in\mathbb R^{n}$ tal que $$\forall v\in\mathbb R^{n},\langle gradf(a),v\rangle:=D_{v}f(a)$$
Notación: $gradf(a)\equiv\nabla f(a),\nabla\equiv\begin{pmatrix}\frac\partial{\partial x_{1}}&\dots&\frac\partial{\partial x_{n}}\end{pmatrix}$ 
Proposición: $f:A\subseteq\mathbb R^{n}\to\mathbb R,a\in A,a\in A',f$ diferenciable en a
1. $\nabla f(a)$ indica la dirección y sentido de máximo crecimiento/decrecimiento de $a$, creciente en el sentido de $\nabla f(a)$ y decreciente en el sentido opuesto $-\nabla f(a)$
	El valor de ese crecimiento/decrecimiento la da la derivada direccional de $f$ en $a$ y ese valor es $\pm||{\nabla f(a)}||$ cuando se calcula con vectores unitarios
2. En cualquier dirección perpendicular a la del $\nabla f(a)$ en $a$, la variación de la función es nula en $E(a)$ 
	Corolario: $\nabla f(a)$ es un vector perpendicular al conjunto de nivel de la función $f$ que pasa por $a$

# Propiedades de las funciones diferenciables
## Primeras propiedades
$f,g:A\subseteq\mathbb R^{n}\to\mathbb R^{m},a\in A,a\in A',f,g$ diferenciables en $a$:
1. $$\forall\alpha,\beta\in\mathbb R\quad D(\alpha f+\beta g)(a)=\alpha Df(a)+\beta Dg(a)$$ 
por lo que es lineal respecto la suma. Expresado en matrices da: $$\mathcal J(\alpha f+\beta g)=\alpha\mathcal Jf(a)+\beta\mathcal Jg(a)$$
2. La regla de Leibnitz para el producto: $fg$ es diferenciable en $a$ y $$D(fg)(a)=Df(a)g(a)+f(a)Dg(a)$$
  que también se puede representar con matrices: 
  3. Regla de Leibnitz para el cuociente: $\frac f g$ es diferenciable en $a$ y da $$D(\frac f g)=\frac 1 {g(a)^{2}}(Df(a)g(a)-f(a)Dg(a))$$ que, una vez más, se puede representar con matrices

# Diferenciabilidad y continuabilidad
$f:A\subseteq\mathbb R^{n}\to\mathbb R^{m},a\in A,a\in A'$
- Si $f$ es diferenciable en $a\implies f$ es continua en $a$ 

Proposición: Si $f$ tiene derivadas parciales definidas en $E(a)$ y son funciones continuas en a$\implies f$ es continua en $a$ 

Definición: $f:A\subseteq \mathbb R^{n}\to\mathbb R^{m}$ es de clase $C^{1}$ si en $D\subseteq A$ existen sus derivadas parciales en todos los puntos de $D$ y son funciones contínues en $D$ 
	Corolario: $f$ es de clase $C^{1}\implies f$ diferenciable $\implies f$ contínua

## Teorema de la función compuesta 
AKA regla de la cadena
$f:A\subseteq\mathbb R^{n}\to\mathbb R^{m},a\in A,a\in A'$ 
$g:B\subseteq\mathbb R^{m}\to\mathbb R^{l}$ tal que $Im(f)\equiv f(A)\subset B$ y $f(a)\in B'$ 
tal que $f$ es diferenciable en $a$ y $g$ es diferenciable en $f(a)\equiv b$ entonces $g\circ f$ es diferenciable en $a$ y $$D(g\circ f)(a)=Dg(f(a))\circ Df(a)$$ que expresado en matrices es $$\mathcal J(g\circ f)(a)=\mathcal Jg(f(a))\mathcal Jf(a)$$
De manera que el diagrama de los espacios es
$$h\equiv g\circ f:A\subseteq\mathbb R^{n}\xrightarrow{f} B\subseteq\mathbb R^{m}\xrightarrow{g}\mathbb R^{l}$$ la de los vectores es
  $$X\equiv(x_{1},\dots,x_{n})\to Y\equiv(y_{1},\dots,y_{n)\to}g(y)\equiv(g(f(x))=(g_{1}(f(x),\dots,g_{l}(x)))$$ y la de los puntos es $$a\to f(a)\equiv b\to g(b)=g(f(a))$$
  Finalmente, esta composición expresada en matrices es: 
    $$\large\begin{pmatrix}\frac{\partial h_{1}}{\partial x_{1}}(a)&\dots&\frac{\partial h_{1}}{\partial x_{n}}(a) \\ 
  \vdots&\frac{\partial h_{i}}{\partial x_{j}}(a)&\vdots \\ 
  \frac{\partial h_{l}}{\partial x_{1}}(a) &\dots&\frac{\partial h_{l}}{\partial x_{n}}(a)\end{pmatrix}
  =
  \begin{pmatrix}\frac{\partial g_{1}}{\partial y_{1}}(a)&\dots&\frac{\partial g_{1}}{\partial y_{m}}(a) \\ 
  \vdots&\frac{\partial g_{i}}{\partial y_{j}}(a)&\vdots \\ 
  \frac{\partial g_{l}}{\partial y_{1}}(a) &\dots&\frac{\partial g_{l}}{\partial y_{m}}(a)
  \end{pmatrix}
  ·
  \begin{pmatrix}
  \frac{\partial f_{1}}{\partial x_{1}}(a)&\dots&\frac{\partial f_{1}}{\partial x_{n}}(a) \\ 
  \vdots&\frac{\partial f_{i}}{\partial x_{j}}(a)&\vdots \\ 
  \frac{\partial f_{m}}{\partial x_{1}}(a) &\dots&\frac{\partial f_{m}}{\partial x_{n}}(a)
  \end{pmatrix}$$
  
   De aquí podemos calcular una componente: $$\frac{\partial h_{i}}{\partial x_{j}}(a)=\frac{\partial g_{i}}{\partial y_{1}}(f(a))\frac{\partial f_{1}}{\partial x_{j}}(a)+\dots+\frac{\partial g_{i}}{\partial y_{m}}(f(a))\frac{\partial f_{m}}{\partial x_{j}}(a)=\sum\limits_{k=1}^{m}\frac{\partial g_{i}}{\partial y_{k}}(f(a))\frac{\partial f_{k}}{\partial x_{j}}(a)$$

# Derivadas parciales de orden superior:
$f:A\subseteq \mathbb R^{n}\to\mathbb R$
$x\equiv(x_{1},\dots,x_{n})\to f(x)$
- La derivada parcial de orden 1/primer orden se define como: $$\frac{\partial f}{\partial x}:A\subseteq\mathbb R^{n}\to R$$  $$a\equiv(a_{1},\dots,a_{n})\to\frac{\partial f}{\partial x_{i}}(a)$$ de los cuales hay n

- La derivada parcial de orden 2/segundo orden se define como: $$\frac\partial{\partial x_{j}}\frac{\partial f}{\partial x_{i}}\equiv\frac{\partial^{2}f}{\partial x_{j}\partial x_{i}}:A\subseteq\mathbb R^{n}\to R$$ $$a\equiv(a_1,\dots,a_{n})\to\frac\partial{\partial x_{j}}\frac{\partial f}{\partial x_{i}}(a)\equiv\frac{\partial^{2}f}{\partial x_{j}\partial x_{i}}(a)$$ de los cuales hay $n^{2}$

- La derivada parcial de orden n/n-ésimo orden se define como: $$\frac{\partial^{m}f}{\partial x_{m}\dotsm\partial x_{i}}:A\subseteq\mathbb R^{n}\to\mathbb R$$ de los cuales hay $n^{m}$

## Matriz Hessiana
Para representar todas las derivadas parciales de segundo orden podemos usar una matriz, la matriz Hessiana de $f$ en $a$
$$
\large\begin{pmatrix}
\frac{\partial^{2}f}{\partial x_{1}^{2}}(a) & \frac{\partial^{2}f}{\partial x_{2}\partial x_{1}}(a) & \dots & \frac{\partial^{2}f}{\partial x_{n}\partial x_{1}}(a) \\ 
\frac{\partial^{}f}{\partial x_{1}\partial x_{2}}(a)&\dots&\dots&\frac{\partial^{2}f}{\partial x_{n}\partial x_{2}}(a) \\ 
\vdots & \vdots & \vdots &\vdots \\ 
\frac{\partial^{2}f}{\partial x_{1}\partial x_{n}}(a) &\dots &\dots & \frac{\partial^{2}f}{\partial x_{n}^{2}}(a)
\end{pmatrix}\equiv Hf(a)
$$
Llamamos al determinante de esta matriz el Hessiano de $f$ en $a$ 

Derivadas cruzadas: En general, $$\frac{\partial^{2}f}{\partial x\partial y}\neq\frac{\partial^{2}f}{\partial y\partial x}(a)$$
# Teorema de Schwarz
$f:A\subseteq\mathbb R^{n}\to\mathbb R$, si las derivadas cruzadas de 2º orden existen en $D\subseteq A$ y son funciones continuas, las derivadas cruzadas son iguales $\forall a\in D\subseteq A$ 

$f$ es de clase $C^{k}$ en $D\subseteq A$ si existen todas las derivadas parciales hasta el orden $k$ y todas son funciones continuas, y si lo es $\forall k\in\mathbb N$, es de clase $C^{\infty}$ 

Proposición:
- $f$ es de clase $C^{k}\implies f$ es de clase $C^{k-1}$
- La combinación lineal de funciones de clase $C^{k}$ es de clase $C^{k}$
- El producto de funciones de clase $C^{k}$ es de clase $C^{k}$
- La composición de funciones de clase $C^{k}$ es de clase $C^{k}$
Corolario: $f$ es de clase $C^{k}\implies$ las derivadas cruzadas de orden $2\leq m\leq k$ son iguales

# Teorema de la función inversa
$f:A\subseteq \mathbb R^{n}\to\mathbb R^{n}, a\in A,a\in A'$ tal que $f$ es de clase $C^{k}(k\geq 1)$ en $a$ 
$x\equiv(x_{1},\dots,x_{n})\to (f_{1}(x),\dots,f_{n}(x))$

La condición necesaria y suficiente para que exista un abierto $U\subset\mathbb R^{n},a\in U$ y un abierto $V\subset \mathbb R^{n},f(a)\in V$ y la inversa diferenciable en $f(a)$: $f^{-1}:V\subseteq\mathbb R^{n}\to U\subseteq\mathbb R^{n}$ es que $det\mathcal J f(a)\neq0$ 

Entonces: $$Df^{-1}(f(a))=(Df(a))^{-1}$$ $$\mathcal Jf^{-1}(f(a))=(\mathcal Jf(a))^{-1}$$
## Difeomorfismo local
$f:A\subseteq\mathbb R^{n}\to\mathbb R^{n}$ es un difeomorfismo local de clase $C^{k}$ si 
1. $f:U\subset A\subseteq\mathbb R^{n}\to V\subseteq\mathbb R^{n}$ es diferenciable de clase $C^{k}$
2. $\exists f^{-1}:V\subset\mathbb R^{n}\to\mathbb R^{n}$ es decir, tiene inversa local
3. $f^{-1}$ es diferenciable de clase $C^{k}$
Si $f$ es inyectiva en su dominio y $Det(f)\neq 0\forall x\in Dom(f)$, la inversa es global

# Teorema de la función implícita
Sea $p\equiv(a,b)\in W\subseteq\mathbb R^{n}\times\mathbb R^{m}$ y $F:W\subseteq\mathbb R^{n+m}\equiv\mathbb R^{n}\times\mathbb R^{m}\to\mathbb R^{m}$ tal que
1. $F$ es diferenciable de clase $C^{k} (k\geq 1)$, en un entorno $E(p)$
2. $F(p)=0$
3. $det(\frac{\partial F}{\partial y_{i}}(p))\neq 0$
Entonces $\exists A\subset\mathbb R^{n}|a\in A,\exists B\subset\mathbb R^{n}|b\in B,\exists!f:A\subset\mathbb R^{n}\to B\subset\mathbb R^{m}$ tal que
1. $F(x_{1},\dots,x_{n};f_{1}(x),\dots,f_{m}(x))=F(x,f(x))=0\quad\forall x\in A$ es decir, $f$ es un trozo de $F$ 
2. $f$ es diferenciable de clase $C^{k}$

## Corolario
$$\mathcal J f(a)=\begin{pmatrix}\frac{\partial f_{k}}{\partial x_{j}}(a)\end{pmatrix}=\begin{pmatrix}\frac{\partial F_{i}}{\partial y_{k}}(p))\end{pmatrix}^{-1}\begin{pmatrix}\frac{\partial F_{i}}{\partial x_{j}}(p)\end{pmatrix}
$$
que, en términos sencillos, es: $$\mathcal Jf(a)=-B^{-1}A$$ donde $B$ es el bloque cuadrado de la matriz Jacobiana de $f$ con las columnas que se quieren aislar y $A$ es lo que queda.

