La informació és la incertesa sobre un experiment, ja que el resultat és probabilístic.

Sigui  $X$ una variable aleatòria, la quantitat d'informació que s'obté del resultat $a$ és $$I=\log\frac1 {p(a)}=-\log(p(a))$$
Si el $\log$ és base 10 les unitats són $Bits$, si són base $e$ les unitats són Nats

- $P(\text{esdeveniment segur})=1\implies I=0$
- $P(\text{esdeveniment impossible})=0\implies I=\infty$

# Entropia
L'entropia d'una variable aleatòria $X$ és l'esperança de la informació que proporcionen tots els resultats d'un experiment: $$H(X)=E[-\log(P(x))]=E[I(x)]=E[\log\frac1 {p(x)}]$$
- A PIE 1 vam veure que $$E(X)=\sum\limits x_{i}p(x_{i})$$
- A TEOI l'esperança passa a ser $$E[f(x)]=\sum\limits f(x_{i})p(x_{i})$$
## Variable discreta
Sigui $X$ una variable aleatòria discreta definida en el conjunt $\mathcal X$, la seva entropia és $$H(X)=E[-\log p(x)]=-\sum\limits_{x\in\mathcal X}p(x)\log p(x)=\sum\limits_{x\in\mathcal X}p(x)\log\frac 1{p(x)}$$
### Distribució uniforme
Sigui $X\sim U(n)$, l'entropia és $$H(x)=\log(n)$$
## Esdeveniment segur
L'entropia de l'esdeveniment segur $P(\text{esdeveniment})=1$ i el de l'esdeveniment impossible $P(esdeveniment)=0$ són $H(X)=0$

## Entropia d'una distribució de probabilitats
Sigui $X$ una variable aleatòria, $H(p)$ depèn exclusivament de $p=p(x_{i}),\quad i=1,\dots,n$ i no dels valors de $X$: $$H(p)=H(p_{1},\dots,p_{n})=-\sum\limits_{i=1}^{n}p_{i}\log p_{i}=\sum\limits_{i=1}^{n}p_{i}\log\frac 1{p_{i}}$$
## Propietats
### Positivitat
$$H(p_{1},\dots,p_{n})\geq 0$$
### Irrellevància de successos impossibles
$$H(p=0)=H(p=1)=0$$
### Simetria
$$H(p_{1},\dots,p_{n})=H(p_{\sigma(1)},\dots,p_{\sigma(n)})\quad\forall\sigma$$ on $\sigma$ és una permutació
# Divergència
Donades dos distribucions de probabilitat $(p_{i})_{i=1\dots n},(q_{i})_{i=1,\dots n}$ la seva divergència és $$D(p||q)=\sum\limits_{i=1}^{n}p_{i}\log\frac{p_{i}}{q_{i}}=E(Z),\quad Z=\\log\frac X Y$$
## Propietats
- $$D(p||q)\neq D(q||p)$$
- $$D(p||q)\geq 0$$
## Lema de Gibbs
Per tot parell de distribucions $(p_{i})_{i=1\dots n},(q_{i})_{i=1,\dots n}$ : $$\sum\limits_{i=1}^{n}p_{i}\log\frac{p_{i}}{q_{i}}\geq 0$$ on la igualtat es compleix si $p_{i}=q_{i}\quad\forall i$ 

### Reformulació de Gibbs
$$\sum\limits_{i=1}^{n}p_{i}\log\frac 1{p_{i}}\leq\sum\limits_{i=1}^{n}p_{i}\log\frac 1{q_{i}}$$
Amb aquestes desigualtats es pot arribar a que $0\geq D(p||q)\leq\infty$ i que $$H(p)\leq\log n\quad p=(p_{1},\dots,p_{n})$$
# Entropia per distribucions multivariables
## $n=2$
Siguin $X,Y$ un parell de variables aleatòries definides en $\mathcal{X, Y}$. L'entropia conjunta és $$H(X,Y)=\sum\limits_{x\in X,y\in Y}p(x,y)\log\frac 1{p(x,y)}=E(-\log p(x, y))=E(\log\frac 1{p(x,y)})$$
## Generalitzant per $n$
$$H(\mathbb X)=H(X_{1},\dots,X_{n})=\sum\limits_{x\in X}p(x_{1},\dots,x_{n})\log\frac 1 {p(x_{1},\dots,x_{n})}$$
## Propietats
$$H(X,Y)\leq H(X)+H(Y)$$ on la igualtat es compleix si $X,Y$ són independents
# Entropia condicionada
Siguin $X,Y$ variables aleatòries sobre el suport $\mathcal{X,Y}$ respectivament.

La variable aleatòria $Y|X=x$ té entropia $$H(Y|X=x)=\sum\limits_{y\in\mathcal Y}p(y|X=x)\log\frac1{p(y|X=x)}$$ que per una $X$ genèrica és $$H(Y|X)=E_{X}[H(Y|X=x_{i}) \quad i=1,\dots,n]=\sum\limits_{x\in\mathcal X}p(x)H(Y|X=x)$$ que s'interpreta com la quantitat d'informació en promig que proporciona saber el valor de $Y$ quan es coneix $X$.
## Propietats
- Regla de la cadena: $$H(X,Y)=H(X)+H(Y|X)=H(Y)+H(X|Y)$$ generalitzant per $n$: $$H(x_{1},\dots,x_{n})=\sum\limits_{k=1}^{n}H(x_{k}|x_{1},\dots,x_{k-1})$$
- $$H(X|Y)\leq H(X)$$
# Variables aleatòries en funció d'altres
Sigui $X\in\mathcal X$ una variable aleatòria i $Y=g(X)$ tal que $$p(X=x,Y=y)=\begin{cases}
p(X=x) & y=g(x) \\
0 & y\neq g(x)
\end{cases}$$
Si $Y$ és funció determinista de $X$: $$H(Y|X=x)=0$$
## Propietats
- Són equivalents:
	- $Y$ és funció determinista de $X$ ($Y=g(X)$)
	- $H(Y|X)=0$
	- $H(X,Y)=H(X)$
- $$H(Y)=H(X)\iff g\text{ és injectiva}$$
# Informació mútua
Informació de la que disposem sobre $X=x$ si observem la variable $Y=y$ $$I(Y=y,X=x)=\log\frac{p(x|y)}{p(x)p(y)}$$
Informació mútua: $$I(X,Y)=E_{X,Y}[\log\frac{p(x,y)}{p(x)p(y)}]=\sum\limits_{x,y\in\{x,y\}}p(x,y)\log{p(x,y)}{p(x)p(y)}$$
## Propietats
- $$I(X,Y)=H(X)-H(Y)-H(X,Y)$$
- $$I(X,Y)=H(X)-H(X|Y)=H(Y)-H(Y|X)$$
- $$I(X,Y)=I(Y,X)$$
- $$I(X,Y)\geq 0,\quad I(X,Y)=0\iff X,Y\text{ independents}$$
- $$I(X,X)=H(X)$$
- $$I(X,Y)\leq H(X),\quad I(X,Y)\leq H(Y)$$
- $$I(X,Y)=D(p(x, y)||p(x)p(y))$$
## Informació mútua condicionada
Siguin $X,Y,Z$ variables aleatòries, $X\in\mathcal X$, $Y\in\mathcal Y$ i $Z\in\mathcal Z$. La informació mutua de $X,Y$ condicionada a $Z$ és $$I(X,Y|Z)=E_{X,Y,Z}\left[\log\frac{p(X,Y|Z)}{p(X|Z)p(Y|Z)}\right]=\sum\limits_{x,y,z\in\{\mathcal{X,Y,Z}\}}p(x,y,z)\log\frac{p(x,y|z)}{p(x|z)p(y|z)}$$ i s'interpreta com la informació que proporciona $X$ sobre $Y$ quan es coneix $Z$
### Propietats
- $$I(X,Y|Z)=H(Y|Z)-H(Y|X,Z)$$
- Simetria: $$I(X,Y|Z)=I(Y,X|Z)$$
- $$I(X,X|Z)=H(X|Z)$$
- $$I(X,Y|Z)\geq 0$$
- Regla de la cadena: $$I(X,Y|Z)=I(X,Z)+I(Y,Z|X)$$ i $I=0\iff$ $X|Z$ i $Y|Z$ són independents.
# Desigualtat de processament de dades
Sigui $X\to Y\to Z$ una cadena de Markov: $$I(X,Z)\leq I(Y,Z)$$ $$I(X,Z)\leq I(X,Y)$$
# Entropia diferencial
Sigui $X$ una variable discreta amb distribució de probabilitats $f_{X}$, l'entropia diferencial és $$h(X)=E[-\log f_{X}]=-\int_{-\infty}^{\infty}f_{X}(x)\log f_{X}(x)dx$$ on $f_{X}(x)\log f_{X}(x)=0\quad\forall x|f_{X}(x)=0$ 
- Per una variable uniforme en $[a,b]$: $$h(X)=\log(b-a)$$
- Per una variable normal: $$h(X)=\frac 1 2\log(2\pi e\sigma^{2})$$
## Per diverses variables
$$h(X,Y)=-\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f_{XY}(x,y)\log f_{XY}(x,y)dxdy$$ $$I(X,Y)=-\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f_{XY}(x,y)\frac{f_{XY}(x,y)}{f_{X}(x)f_{Y}(y)}$$
- En una cadena: $$h(X,Y)=h(X)+h(Y|X)=h(Y)+h(X|Y)$$ $$I(X,Y)=h(Y)-h(Y|X)=h(X)-h(X|Y)=h(X,Y)-h(X)-h(Y)$$ on $h(X),I(X,Y)$ poden ser negatius

# Processos estocàstics
L'entropia de $X=(X_{n})_{n\geq 1}$ és la taxa d'entropia del procés, calculat de dos maneres
- Mètode 1
$$H(X)=\lim_{n\to\infty} \frac{H(X_{1},\dots,X_{n})}n$$
- Mètode 2
$$H(X)=\lim_{n\to\infty}H(X_{n}|X_{1},\dots,X_{n-1})$$
Aquests mètodes no sempre existeixen, ja que un procés pot no tenir definida una taxa d'entropia, però si existeixen, els dos mètodes han de donar el mateix.

Altres casos possibles son que existeixi el mètode 1 i el 2 no o viceversa. 

# Processos estacionaris
Un procés estacionari que compleix $P(X_{1},\dots,X_{n})=P(X_{1+i},\dots,X_{n+i})$ té sempre el límit del mètode 2 definit.
# Cadena de Markov estacionària
$$P(X_{n+1}=x_{n+1}|X_{n}=x_{n},\dots,X_{1}=x_{1})=P(X_{n+1}=x_{n+1}|X_{n}=x_{n})\quad\forall n\in\mathbb N$$
## Taxa d'entropia
$$H(X)=H(X_{2}|X_{1})\implies H(X)=-\sum\limits_{x_{1},x_{2}}p(X_{1}=x_{1},X_{2}=x_{2})\log p(X_{2}=x_{2}|X_{1}=x_{1})$$
