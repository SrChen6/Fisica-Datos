La informació és la incertesa sobre un experiment, ja que el resultat és probabilístic.

Sigui  $X$ una variable aleatòria, la quantitat d'informació que s'obté del resultat $a$ és $$I=\log\frac1 {p(a)}=-\log(p(a))$$
Si el $\log$ és base 10 les unitats són $Bits$, si són base $e$ les unitats són Nats

- $P(\text{esdeveniment segur})=1\implies I=0$
- $P(\text{esdeveniment impossible})=0\implies I=\infty$

# Entropia
L'entropia d'una variable aleatòria $X$ és l'esperança de la informació que proporcionen tots els resultats d'un experiment: $$H(X)=E[-\log(P(x))]=E[I(x)]=E[\log\frac1 {p(x)}]$$
- A PIE 1 vam veure que $$E(X)=\sum\limits x_{i}p(x_{i})$$
- A TEOI l'esperança passa a ser $$E[f(x)]=\sum\limits f(x_{i})p(x_{i})$$
## Variable discreta
Sigui $X$ una variable aleatòria discreta definida en el conjunt $\mathcal X$, la seva entropia és $$H(X)=E[-\log p(x)]=-\sum\limits_{x\in\mathcal X}p(x)\log p(x)=\sum\limits_{x\in\mathcal X}p(x)\log\frac 1{p(x)}$$
### Distribució uniforme
Sigui $X\sim U(n)$, l'entropia és $$H(x)=\log(n)$$
## Esdeveniment segur
L'entropia de l'esdeveniment segur $P(\text{esdeveniment})=1$ i el de l'esdeveniment impossible $P(esdeveniment)=0$ són $H(X)=0$

## Entropia d'una distribució de probabilitats
Sigui $X$ una variable aleatòria, $H(p)$ depèn exclusivament de $p=p(x_{i}),\quad i=1,\dots,n$ i no dels valors de $X$: $$H(p)=H(p_{1},\dots,p_{n})=-\sum\limits_{i=1}^{n}p_{i}\log p_{i}=\sum\limits_{i=1}^{n}p_{i}\log\frac 1{p_{i}}$$
## Propietats
### Positivitat
$$H(p_{1},\dots,p_{n})\geq 0$$
### Irrellevància de successos impossibles
$$H(p=0)=H(p=1)=0$$
### Simetria
$$H(p_{1},\dots,p_{n})=H(p_{\sigma(1)},\dots,p_{\sigma(n)})\quad\forall\sigma$$ on $\sigma$ és una permutació
# Divergència
Donades dos distribucions de probabilitat $(p_{i})_{i=1\dots n},(q_{i})_{i=1,\dots n}$ la seva divergència és $$D(p||q)=\sum\limits_{i=1}^{n}p_{i}\log\frac{p_{i}}{q_{i}}=E(Z),\quad Z=\\log\frac X Y$$
## Propietats
- $$D(p||q)\neq D(q||p)$$
- $$D(p||q)\geq 0$$
## Lema de Gibbs
Per tot parell de distribucions $(p_{i})_{i=1\dots n},(q_{i})_{i=1,\dots n}$ : $$\sum\limits_{i=1}^{n}p_{i}\log\frac{p_{i}}{q_{i}}\geq 0$$ on la igualtat es compleix si $p_{i}=q_{i}\quad\forall i$ 

### Reformulació de Gibbs
$$\sum\limits_{i=1}^{n}p_{i}\log\frac 1{p_{i}}\leq\sum\limits_{i=1}^{n}p_{i}\log\frac 1{q_{i}}$$
Amb aquestes desigualtats es pot arribar a que $0\geq D(p||q)\leq\infty$ i que $$H(p)\leq\log n\quad p=(p_{1},\dots,p_{n})$$
# Entropia per distribucions multivariables
## $n=2$
Siguin $X,Y$ un parell de variables aleatòries definides en $\mathcal{X, Y}$. L'entropia conjunta és $$H(X,Y)=\sum\limits_{x\in X,y\in Y}p(x,y)\log\frac 1{p(x,y)}=E(-\log p(x, y))=E(\log\frac 1{p(x,y)})$$
## Generalitzant per $n$
$$H(\mathbb X)=H(X_{1},\dots,X_{n})=\sum\limits_{x\in X}p(x_{1},\dots,x_{n})\log\frac 1 {p(x_{1},\dots,x_{n})}$$
## Propietats
$$H(X,Y)\leq H(X)+H(Y)$$ on la igualtat es compleix si $X,Y$ són independents
