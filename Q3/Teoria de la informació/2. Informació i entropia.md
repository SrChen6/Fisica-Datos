La informació és la incertesa sobre un experiment, ja que el resultat és probabilístic.

Sigui  $X$ una variable aleatòria, la quantitat d'informació que s'obté del resultat $a$ és $$I=\log\frac1 {p(a)}=-\log(p(a))$$
Si el $\log$ és base 10 les unitats són $Bits$, si són base $e$ les unitats són Nats

- $P(\text{esdeveniment segur})=1\implies I=0$
- $P(\text{esdeveniment impossible})=0\implies I=\infty$

# Entropia
L'entropia d'una variable aleatòria $X$ és l'esperança de la informació que proporcionen tots els resultats d'un experiment: $$H(X)=E[-\log(P(x))]=E[I(x)]=E[\log\frac1 {p(x)}]$$
- A PIE 1 vam veure que $$E(X)=\sum\limits x_{i}p(x_{i})$$
- A TEOI l'esperança passa a ser $$E[f(x)]=\sum\limits f(x_{i})p(x_{i})$$
## Variable discreta
Sigui $X$ una variable aleatòria discreta definida en el conjunt $\mathcal X$, la seva entropia és $$H(X)=E[-\log p(x)]=-\sum\limits_{x\in\mathcal X}p(x)\log p(x)=\sum\limits_{x\in\mathcal X}p(x)\log\frac 1{p(x)}$$
### Distribució uniforme
Sigui $X\sim U(n)$, l'entropia és $$H(x)=\log(n)$$
## Esdeveniment segur
L'entropia de l'esdeveniment segur $P(\text{esdeveniment})=1$ i el de l'esdeveniment impossible $P(esdeveniment)=0$ són $H(X)=0$

## Entropia d'una distribució de probabilitats
Sigui $X$ una variable aleatòria, $H(p)$ depèn exclusivament de $p=p(x_{i}),\quad i=1,\dots,n$ i no dels valors de $X$: $$H(p)=H(p_{1},\dots,p_{n})=-\sum\limits_{i=1}^{n}p_{i}\log p_{i}=\sum\limits_{i=1}^{n}p_{i}\log\frac 1{p_{i}}$$
## Propietats
### Positivitat
$$H(p_{1},\dots,p_{n})\geq 0$$
### Irrellevància de successos impossibles
$$H(p=0)=H(p=1)=0$$
### Simetria
$$H(p_{1},\dots,p_{n})=H(p_{\sigma(1)},\dots,p_{\sigma(n)})\quad\forall\sigma$$ on $\sigma$ és una permutació
# Divergència
Donades dos distribucions de probabilitat $(p_{i})_{i=1\dots n},(q_{i})_{i=1,\dots n}$ la seva divergència és $$D(p||q)=\sum\limits_{i=1}^{n}p_{i}\log\frac{p_{i}}{q_{i}}=E(Z),\quad Z=\\log\frac X Y$$
## Propietats
- $$D(p||q)\neq D(q||p)$$
- $$D(p||q)\geq 0$$
## Lema de Gibbs
Per tot parell de distribucions $(p_{i})_{i=1\dots n},(q_{i})_{i=1,\dots n}$ : $$\sum\limits_{i=1}^{n}p_{i}\log\frac{p_{i}}{q_{i}}\geq 0$$ on la igualtat es compleix si $p_{i}=q_{i}\quad\forall i$ 

### Reformulació de Gibbs
$$\sum\limits_{i=1}^{n}p_{i}\log\frac 1{p_{i}}\leq\sum\limits_{i=1}^{n}p_{i}\log\frac 1{q_{i}}$$
Amb aquestes desigualtats es pot arribar a que $0\geq D(p||q)\leq\infty$ i que $$H(p)\leq\log n\quad p=(p_{1},\dots,p_{n})$$
# Entropia per distribucions multivariables
## $n=2$
Siguin $X,Y$ un parell de variables aleatòries definides en $\mathcal{X, Y}$. L'entropia conjunta és $$H(X,Y)=\sum\limits_{x\in X,y\in Y}p(x,y)\log\frac 1{p(x,y)}=E(-\log p(x, y))=E(\log\frac 1{p(x,y)})$$
## Generalitzant per $n$
$$H(\mathbb X)=H(X_{1},\dots,X_{n})=\sum\limits_{x\in X}p(x_{1},\dots,x_{n})\log\frac 1 {p(x_{1},\dots,x_{n})}$$
## Propietats
$$H(X,Y)\leq H(X)+H(Y)$$ on la igualtat es compleix si $X,Y$ són independents
# Entropia condicionada
Siguin $X,Y$ variables aleatòries sobre el suport $\mathcal{X,Y}$ respectivament.

La variable aleatòria $Y|X=x$ té entropia $$H(Y|X=x)=\sum\limits_{y\in\mathcal Y}p(y|X=x)\log\frac1{p(y|X=x)}$$ que per una $X$ genèrica és $$H(Y|X)=E_{X}[H(Y|X=x_{i}) \quad i=1,\dots,n]=\sum\limits_{x\in\mathcal X}p(x)\sum\limits_{y\in\mathcal Y}p(y|x)\log\frac 1{p(y|x)}$$ que s'interpreta com la quantitat d'informació en promig que proporciona saber el valor de $Y$ quan es coneix $X$.
## Propietats
- Regla de la cadena: $$H(X,Y)=H(X)+H(Y|X)=H(Y)+H(X|Y)$$ generalitzant per $n$: $$H(x_{1},\dots,x_{n})=\sum\limits_{k=1}^{n}H(x_{k}|x_{1},\dots,x_{k-1})$$
- $$H(X|Y)\leq H(X)$$
# Variables aleatòries en funció d'altres
Sigui $X\in\mathcal X$ una variable aleatòria i $Y=g(X)$ tal que $$p(X=x,Y=y)=\begin{cases}
p(X=x) & y=g(x) \\
0 & y\neq g(x)
\end{cases}$$
Si $Y$ és funció determinista de $X$: $$H(Y|X=x)=0$$
## Propietats
- Són equivalents:
	- $Y$ és funció determinista de $X$ ($Y=g(X)$)
	- $H(Y|X)=0$
	- $H(X,Y)=H(X)$
- $$H(Y)=H(X)\iff g\text{ és injectiva}$$
# Informació mútua
Informació de la que disposem sobre $X=x$ si observem la variable $Y=y$ $$I(Y=y,X=x)=\log\frac{p(x|y)}{p(x)p(y)}$$
Informació mútua: $$I(X,Y)=E_{X,Y}[\log\frac{p(x,y)}{p(x)p(y)}]=\sum\limits_{x,y\in\{x,y\}}p(x,y)\log{p(x,y)}{p(x)p(y)}$$
## Propietats
- $$I(X,Y)=H(X)-H(Y)-H(X,Y)$$
- $$I(X,Y)=H(X)-H(X|Y)=H(Y)-H(Y|X)$$
- $$I(X,Y)=I(Y,X)$$
- $$I(X,Y)\geq 0,\quad I(X,Y)=0\iff X,Y\text{ independents}$$
- $$I(X,X)=H(X)$$
- $$I(X,Y)\leq H(X),\quad I(X,Y)\leq H(Y)$$
- $$I(X,Y)=D(p(x, y)||p(x)p(y))$$
## Informació mútua condicionada
Siguin $X,Y,Z$ variables aleatòries, $X\in\mathcal X$, $Y\in\mathcal Y$ i $Z\in\mathcal Z$. La informació mutua de $X,Y$ condicionada a $Z$ és $$I(X,Y|Z)=E_{X,Y,Z}\left[\log\frac{p(X,Y|Z)}{p(X|Z)p(Y|Z)}\right]=\sum\limits_{x,y,z\in\{\mathcal{X,Y,Z}\}}p(x,y,z)\log\frac{p(x,y|z)}{p(x|z)p(y|z)}$$ i s'interpreta com la informació que proporciona $X$ sobre $Y$ quan es coneix $Z$
### Propietats
- $$I(X,Y|Z)=H(Y|Z)-H(Y|X,Z)$$
- Simetria: $$I(X,Y|Z)=I(Y,X|Z)$$
- $$I(X,X|Z)=H(X|Z)$$
- $$I(X,Y|Z)\geq 0$$
- Regla de la cadena: $$I(X,Y|Z)=I(X,Z)+I(Y,Z|X)$$ i $I=0\iff$ $X|Z$ i $Y|Z$ són independents.
# Desigualtat de processament de dades
Sigui $X\to Y\to Z$ una cadena de Markov: $$I(X,Z)\leq I(Y,Z)$$